<!DOCTYPE html>
<html lang="en">

  <!-- Head -->
  <head>    <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Publications | Tuan Le Duc Anh</title>
    <meta name="author" content="Tuan  Le Duc Anh" />
    <meta name="description" content="(*) denotes equal contribution" />
    <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website" />


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous" />

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="none" id="highlight_theme_light" />

    <!-- Styles -->
    
    <link rel="shortcut icon" href="/assets/img//assets/img/favicon.ico"/>
    
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="canonical" href="http://localhost:4000/publications/">
    
    <!-- Dark Mode -->
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark" />

    <script src="/assets/js/theme.js"></script>
    <script src="/assets/js/dark_mode.js"></script>
    

  </head>

  <!-- Body -->
  <body class="fixed-top-nav ">

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          <a class="navbar-brand title font-weight-lighter" href="/">Tuan Le Duc Anh</a>
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <li class="nav-item ">
                <a class="nav-link" href="/">About</a>
              </li>
              
              <!-- Blog -->
              <li class="nav-item ">
                <a class="nav-link" href="/blog/">Blog</a>
              </li>

              <!-- Other pages -->
              <li class="nav-item active">
                <a class="nav-link" href="/publications/">Publications<span class="sr-only">(current)</span></a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/cv/">CV</a>
              </li>

              <!-- Toogle theme mode -->
              <li class="toggle-container">
                <button id="light-toggle" title="Change theme">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
                </button>
              </li>
            </ul>
          </div>
        </div>
      </nav>
    </header>

    <!-- Content -->
    <div class="container mt-5">
      <!-- page.html -->
        <div class="post">

          <header class="post-header">
            <h1 class="post-title">Publications</h1>
            <p class="post-description">(*) denotes equal contribution</p>
          </header>

          <article>
            <!-- _pages/publications.md -->
<div class="publications">
  <h2 class="year">2022</h2>
  <ol class="bibliography">
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">Preprint</abbr></div>

        <!-- Entry bib key -->
        <div id="nguyen2022hsw" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Hierarchical Sliced Wasserstein Distance</div>
          <!-- Author -->
          <div class="author">
          

          Khai Nguyen, Tongzheng Ren, Huy Nguyen, and
            <span class="more-authors" title="click to view 3 more authors" onclick="
                  var element = $(this);
                  element.attr('title', '');
                  var more_authors_text = element.text() == '3 more authors' ? 'Litu Rout, Tan Nguyen, Nhat Ho' : '3 more authors';
                  var cursorPosition = 0;
                  var textAdder = setInterval(function(){
                    element.text(more_authors_text.substring(0, cursorPosition + 1));
                    if (++cursorPosition == more_authors_text.length){
                      clearInterval(textAdder);
                    }
                }, '10');
                ">3 more authors</span>
</div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>arXiv preprint arXiv:2209.13570</em> 2022
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="https://arxiv.org/pdf/2209.13570.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
            <a href="https://github.com/UT-Austin-Data-Science-Group/HSW" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Sliced Wasserstein (SW) distance has been widely used in different application scenarios since it can be scaled to a large number of supports without suffering from the curse of dimensionality. The value of sliced Wasserstein distance is the average of transportation cost between one-dimensional representations (projections) of original measures that are obtained by Radon Transform (RT). Despite its efficiency in the number of supports, estimating the sliced Wasserstein requires a relatively large number of projections in high-dimensional settings. Therefore, for applications where the number of supports is relatively small compared with the dimension, e.g., several deep learning applications where the mini-batch approaches are utilized, the complexities from matrix multiplication of Radon Transform become the main computational bottleneck. To address this issue, we propose to derive projections by linearly and randomly combining a smaller number of projections which are named bottleneck projections. We explain the usage of these projections by introducing Hierarchical Radon Transform (HRT) which is constructed by applying Radon Transform variants recursively. We then formulate the approach into a new metric between measures, named Hierarchical Sliced Wasserstein (HSW) distance. By proving the injectivity of HRT, we derive the metricity of HSW. Moreover, we investigate the theoretical properties of HSW including its connection to SW variants and its computational and sample complexities. Finally, we compare the computational cost and generative quality of HSW with the conventional SW on the task of deep generative modeling using various benchmark datasets including CIFAR10, CelebA, and Tiny ImageNet.</p>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#0048BA"><a href="https://neurips.cc/" target="_blank" rel="noopener noreferrer">NeurIPS</a></abbr></div>

        <!-- Entry bib key -->
        <div id="nguyen2022revisiting" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Revisiting Sliced Wasserstein on Images: From Vectorization to Convolution</div>
          <!-- Author -->
          <div class="author">
          

          Khai Nguyen, and <a href="https://nhatptnk8912.github.io/" target="_blank" rel="noopener noreferrer">Nhat Ho</a>
</div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>Advances in Neural Information Processing Systems</em> 2022
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="https://arxiv.org/pdf/2204.01188.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
            <a href="https://github.com/UT-Austin-Data-Science-Group/CSW" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>The conventional sliced Wasserstein is defined between two probability measures that have realizations as vectors. When comparing two probability measures over images, practitioners first need to vectorize images and then project them to one-dimensional space by using matrix multiplication between the sample matrix and the projection matrix. After that, the sliced Wasserstein is evaluated by averaging the two corresponding one-dimensional projected probability measures. However, this approach has two limitations. The first limitation is that the spatial structure of images is not captured efficiently by the vectorization step; therefore, the later slicing process becomes harder to gather the discrepancy information. The second limitation is memory inefficiency since each slicing direction is a vector that has the same dimension as the images. To address these limitations, we propose novel slicing methods for sliced Wasserstein between probability measures over images that are based on the convolution operators. We derive convolution sliced Wasserstein (CSW) and its variants via incorporating stride, dilation, and non-linear activation function into the convolution operators. We investigate the metricity of CSW as well as its sample complexity, its computational complexity, and its connection to conventional sliced Wasserstein distances. Finally, we demonstrate the favorable performance of CSW over the conventional sliced Wasserstein in comparing probability measures over images and in training deep generative modeling on images.</p>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#0048BA"><a href="https://neurips.cc/" target="_blank" rel="noopener noreferrer">NeurIPS</a></abbr></div>

        <!-- Entry bib key -->
        <div id="nguyen2022amortized" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Amortized Projection Optimization for Sliced Wasserstein Generative Models</div>
          <!-- Author -->
          <div class="author">
          

          Khai Nguyen, and <a href="https://nhatptnk8912.github.io/" target="_blank" rel="noopener noreferrer">Nhat Ho</a>
</div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>Advances in Neural Information Processing Systems</em> 2022
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="https://arxiv.org/pdf/2203.13417.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
            <a href="https://github.com/UT-Austin-Data-Science-Group/AmortizedSW" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>The conventional sliced Wasserstein is defined between two probability measures that have realizations as vectors. When comparing two probability measures over images, practitioners first need to vectorize images and then project them to one-dimensional space by using matrix multiplication between the sample matrix and the projection matrix. After that, the sliced Wasserstein is evaluated by averaging the two corresponding one-dimensional projected probability measures. However, this approach has two limitations. The first limitation is that the spatial structure of images is not captured efficiently by the vectorization step; therefore, the later slicing process becomes harder to gather the discrepancy information. The second limitation is memory inefficiency since each slicing direction is a vector that has the same dimension as the images. To address these limitations, we propose novel slicing methods for sliced Wasserstein between probability measures over images that are based on the convolution operators. We derive convolution sliced Wasserstein (CSW) and its variants via incorporating stride, dilation, and non-linear activation function into the convolution operators. We investigate the metricity of CSW as well as its sample complexity, its computational complexity, and its connection to conventional sliced Wasserstein distances. Finally, we demonstrate the favorable performance of CSW over the conventional sliced Wasserstein in comparing probability measures over images and in training deep generative modeling on images.</p>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#0048BA"><a href="https://neurips.cc/" target="_blank" rel="noopener noreferrer">NeurIPS</a></abbr></div>

        <!-- Entry bib key -->
        <div id="nguyen2022transformer" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Transformer with Fourier Integral Attentions</div>
          <!-- Author -->
          <div class="author">
          

          <a href="https://tanmnguyen89.github.io/" target="_blank" rel="noopener noreferrer">Tan Nguyen</a>, Minh Pham, Tam Nguyen, and
            <span class="more-authors" title="click to view 3 more authors" onclick="
                  var element = $(this);
                  element.attr('title', '');
                  var more_authors_text = element.text() == '3 more authors' ? 'Khai Nguyen, Stanley J Osher, Nhat Ho' : '3 more authors';
                  var cursorPosition = 0;
                  var textAdder = setInterval(function(){
                    element.text(more_authors_text.substring(0, cursorPosition + 1));
                    if (++cursorPosition == more_authors_text.length){
                      clearInterval(textAdder);
                    }
                }, '10');
                ">3 more authors</span>
</div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>Advances in Neural Information Processing Systems</em> 2022
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="https://arxiv.org/pdf/2206.00206.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Multi-head attention empowers the recent success of transformers, the state-of-the-art models that have achieved remarkable success in sequence modeling and beyond. These attention mechanisms compute the pairwise dot products between the queries and keys, which results from the use of unnormalized Gaussian kernels with the assumption that the queries follow a mixture of Gaussian distribution. There is no guarantee that this assumption is valid in practice. In response, we first interpret attention in transformers as a nonparametric kernel regression. We then propose the FourierFormer, a new class of transformers in which the dot-product kernels are replaced by the novel generalized Fourier integral kernels. Different from the dot-product kernels, where we need to choose a good covariance matrix to capture the dependency of the features of data, the generalized Fourier integral kernels can automatically capture such dependency and remove the need to tune the covariance matrix. We theoretically prove that our proposed Fourier integral kernels can efficiently approximate any key and query distributions. Compared to the conventional transformers with dot-product attention, FourierFormers attain better accuracy and reduce the redundancy between attention heads. We empirically corroborate the advantages of FourierFormers over the baseline transformers in a variety of practical applications including language modeling and image classification.</p>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#0048BA"><a href="https://neurips.cc/" target="_blank" rel="noopener noreferrer">NeurIPS</a></abbr></div>

        <!-- Entry bib key -->
        <div id="nguyen2022mix" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Improving Transformer with an Admixture of Attention Heads</div>
          <!-- Author -->
          <div class="author">
          

          <a href="https://tanmnguyen89.github.io/" target="_blank" rel="noopener noreferrer">Tan Nguyen</a>, Tam Nguyen, Hai Do, and
            <span class="more-authors" title="click to view 6 more authors" onclick="
                  var element = $(this);
                  element.attr('title', '');
                  var more_authors_text = element.text() == '6 more authors' ? 'Khai Nguyen, Vishwanath Saragadam, Minh Pham, Khuong Nguyen, Nhat Ho, Stanley J Osher' : '6 more authors';
                  var cursorPosition = 0;
                  var textAdder = setInterval(function(){
                    element.text(more_authors_text.substring(0, cursorPosition + 1));
                    if (++cursorPosition == more_authors_text.length){
                      clearInterval(textAdder);
                    }
                }, '10');
                ">6 more authors</span>
</div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>Advances in Neural Information Processing Systems</em> 2022
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Multi-head attention empowers the recent success of transformers, the state-of-the-art models that have achieved remarkable success in sequence modeling and beyond. These attention mechanisms compute the pairwise dot products between the queries and keys, which results from the use of unnormalized Gaussian kernels with the assumption that the queries follow a mixture of Gaussian distribution. There is no guarantee that this assumption is valid in practice. In response, we first interpret attention in transformers as a nonparametric kernel regression. We then propose the FourierFormer, a new class of transformers in which the dot-product kernels are replaced by the novel generalized Fourier integral kernels. Different from the dot-product kernels, where we need to choose a good covariance matrix to capture the dependency of the features of data, the generalized Fourier integral kernels can automatically capture such dependency and remove the need to tune the covariance matrix. We theoretically prove that our proposed Fourier integral kernels can efficiently approximate any key and query distributions. Compared to the conventional transformers with dot-product attention, FourierFormers attain better accuracy and reduce the redundancy between attention heads. We empirically corroborate the advantages of FourierFormers over the baseline transformers in a variety of practical applications including language modeling and image classification.</p>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#848482"><a href="https://icml.cc/" target="_blank" rel="noopener noreferrer">ICML</a></abbr></div>

        <!-- Entry bib key -->
        <div id="pmlr-v162-nguyen22e" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Improving Mini-batch Optimal Transport via Partial Transportation</div>
          <!-- Author -->
          <div class="author">
          

          Khai Nguyen, Dang Nguyen, The-Anh Vu Le, and
            <span class="more-authors" title="click to view 2 more authors" onclick="
                  var element = $(this);
                  element.attr('title', '');
                  var more_authors_text = element.text() == '2 more authors' ? 'Tung Pham, Nhat Ho' : '2 more authors';
                  var cursorPosition = 0;
                  var textAdder = setInterval(function(){
                    element.text(more_authors_text.substring(0, cursorPosition + 1));
                    if (++cursorPosition == more_authors_text.length){
                      clearInterval(textAdder);
                    }
                }, '10');
                ">2 more authors</span>
</div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>In Proceedings of the 39th International Conference on Machine Learning</em> 17–23 jul 2022
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="https://proceedings.mlr.press/v162/nguyen22e/nguyen22e.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
            <a href="https://github.com/UT-Austin-Data-Science-Group/Mini-batch-OT" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Mini-batch optimal transport (m-OT) has been widely used recently to deal with the memory issue of OT in large-scale applications. Despite their practicality, m-OT suffers from misspecified mappings, namely, mappings that are optimal on the mini-batch level but are partially wrong in the comparison with the optimal transportation plan between the original measures. Motivated by the misspecified mappings issue, we propose a novel mini-batch method by using partial optimal transport (POT) between mini-batch empirical measures, which we refer to as mini-batch partial optimal transport (m-POT). Leveraging the insight from the partial transportation, we explain the source of misspecified mappings from the m-OT and motivate why limiting the amount of transported masses among mini-batches via POT can alleviate the incorrect mappings. Finally, we carry out extensive experiments on various applications such as deep domain adaptation, partial domain adaptation, deep generative model, color transfer, and gradient flow to demonstrate the favorable performance of m-POT compared to current mini-batch methods.</p>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#848482"><a href="https://icml.cc/" target="_blank" rel="noopener noreferrer">ICML</a></abbr></div>

        <!-- Entry bib key -->
        <div id="pmlr-v162-nguyen22d" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">On Transportation of Mini-batches: A Hierarchical Approach</div>
          <!-- Author -->
          <div class="author">
          

          Khai Nguyen, Dang Nguyen, Quoc Nguyen, and
            <span class="more-authors" title="click to view 5 more authors" onclick="
                  var element = $(this);
                  element.attr('title', '');
                  var more_authors_text = element.text() == '5 more authors' ? 'Tung Pham, Hung Bui, Dinh Phung, Trung Le, Nhat Ho' : '5 more authors';
                  var cursorPosition = 0;
                  var textAdder = setInterval(function(){
                    element.text(more_authors_text.substring(0, cursorPosition + 1));
                    if (++cursorPosition == more_authors_text.length){
                      clearInterval(textAdder);
                    }
                }, '10');
                ">5 more authors</span>
</div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>In Proceedings of the 39th International Conference on Machine Learning</em> 17–23 jul 2022
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="https://proceedings.mlr.press/v162/nguyen22d/nguyen22d.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
            <a href="https://github.com/UT-Austin-Data-Science-Group/Mini-batch-OT" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Mini-batch optimal transport (m-OT) has been successfully used in practical applications that involve probability measures with a very high number of supports. The m-OT solves several smaller optimal transport problems and then returns the average of their costs and transportation plans. Despite its scalability advantage, the m-OT does not consider the relationship between mini-batches which leads to undesirable estimation. Moreover, the m-OT does not approximate a proper metric between probability measures since the identity property is not satisfied. To address these problems, we propose a novel mini-batch scheme for optimal transport, named Batch of Mini-batches Optimal Transport (BoMb-OT), that finds the optimal coupling between mini-batches and it can be seen as an approximation to a well-defined distance on the space of probability measures. Furthermore, we show that the m-OT is a limit of the entropic regularized version of the BoMb-OT when the regularized parameter goes to infinity. Finally, we carry out experiments on various applications including deep generative models, deep domain adaptation, approximate Bayesian computation, color transfer, and gradient flow to show that the BoMb-OT can be widely applied and performs well in various applications.</p>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#EFDECD"><a href="http://aistats.org/" target="_blank" rel="noopener noreferrer">AISTATS</a></abbr></div>

        <!-- Entry bib key -->
        <div id="pmlr-v151-le22a" class="col-sm-8">
        
          <!-- Title -->
          <div class="title"> On Multimarginal Partial Optimal Transport: Equivalent Forms and Computational Complexity </div>
          <!-- Author -->
          <div class="author">
          

          Khang Le, Huy Nguyen, Khai Nguyen, and
            <span class="more-authors" title="click to view 2 more authors" onclick="
                  var element = $(this);
                  element.attr('title', '');
                  var more_authors_text = element.text() == '2 more authors' ? 'Tung Pham, Nhat Ho' : '2 more authors';
                  var cursorPosition = 0;
                  var textAdder = setInterval(function(){
                    element.text(more_authors_text.substring(0, cursorPosition + 1));
                    if (++cursorPosition == more_authors_text.length){
                      clearInterval(textAdder);
                    }
                }, '10');
                ">2 more authors</span>
</div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>In Proceedings of The 25th International Conference on Artificial Intelligence and Statistics</em> 28–30 mar 2022
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://proceedings.mlr.press/v151/le22a/le22a.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
          </div>

          
        </div>
      </div>
</li>
</ol>

  <h2 class="year">2021</h2>
  <ol class="bibliography">
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#B284BE"><a href="https://iclr.cc/" target="_blank" rel="noopener noreferrer">ICLR</a></abbr></div>

        <!-- Entry bib key -->
        <div id="nguyen2021distributional" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Distributional Sliced-Wasserstein and Applications to Generative Modeling</div>
          <!-- Author -->
          <div class="author">
          

          Khai Nguyen, <a href="https://nhatptnk8912.github.io/" target="_blank" rel="noopener noreferrer">Nhat Ho</a>, Tung Pham, and
            <span class="more-authors" title="click to view 1 more author" onclick="
                  var element = $(this);
                  element.attr('title', '');
                  var more_authors_text = element.text() == '1 more author' ? 'Hung Bui' : '1 more author';
                  var cursorPosition = 0;
                  var textAdder = setInterval(function(){
                    element.text(more_authors_text.substring(0, cursorPosition + 1));
                    if (++cursorPosition == more_authors_text.length){
                      clearInterval(textAdder);
                    }
                }, '10');
                ">1 more author</span>
</div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>In International Conference on Learning Representations</em> 28–30 mar 2021
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="https://openreview.net/pdf?id=QYjO70ACDK" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
            <a href="https://github.com/VinAIResearch/DSW" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Sliced-Wasserstein distance (SW) and its variant, Max Sliced-Wasserstein distance (Max-SW), have been used widely in the recent years due to their fast computation and scalability even when the probability measures lie in a very high dimensional space. However, SW requires many unnecessary projection samples to approximate its value while Max-SW only uses the most important projection, which ignores the information of other useful directions. In order to account for these weaknesses, we propose a novel distance, named Distributional Sliced-Wasserstein distance (DSW), that finds an optimal distribution over projections that can balance between exploring distinctive projecting directions and the informativeness of projections themselves. We show that the DSW is a generalization of Max-SW, and it can be computed efficiently by searching for the optimal push-forward measure over a set of probability measures over the unit sphere satisfying certain regularizing constraints that favor distinct directions. Finally, we conduct extensive experiments with large-scale datasets to demonstrate the favorable performances of the proposed distances over the previous sliced-based distances in generative modeling applications.</p>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#B284BE"><a href="https://iclr.cc/" target="_blank" rel="noopener noreferrer">ICLR</a></abbr></div>

        <!-- Entry bib key -->
        <div id="nguyen2021improving" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Improving Relational Regularized Autoencoders with Spherical Sliced Fused Gromov Wasserstein</div>
          <!-- Author -->
          <div class="author">
          

          Khai Nguyen, Son Nguyen, <a href="https://nhatptnk8912.github.io/" target="_blank" rel="noopener noreferrer">Nhat Ho</a>, and
            <span class="more-authors" title="click to view 2 more authors" onclick="
                  var element = $(this);
                  element.attr('title', '');
                  var more_authors_text = element.text() == '2 more authors' ? 'Tung Pham, Hung Bui' : '2 more authors';
                  var cursorPosition = 0;
                  var textAdder = setInterval(function(){
                    element.text(more_authors_text.substring(0, cursorPosition + 1));
                    if (++cursorPosition == more_authors_text.length){
                      clearInterval(textAdder);
                    }
                }, '10');
                ">2 more authors</span>
</div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>In International Conference on Learning Representations</em> 28–30 mar 2021
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="https://openreview.net/pdf?id=DiQD7FWL233" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Relational regularized autoencoder (RAE) is a framework to learn the distribution of data by minimizing a reconstruction loss together with a relational regularization on the prior of latent space. A recent attempt to reduce the inner discrepancy between the prior and aggregated posterior distributions is to incorporate sliced fused Gromov-Wasserstein (SFG) between these distributions. That approach has a weakness since it treats every slicing direction similarly, meanwhile several directions are not useful for the discriminative task. To improve the discrepancy and consequently the relational regularization, we propose a new relational discrepancy, named spherical sliced fused Gromov Wasserstein (SSFG), that can find an important area of projections characterized by a von Mises-Fisher distribution. Then, we introduce two variants of SSFG to improve its performance. The first variant, named mixture spherical sliced fused Gromov Wasserstein (MSSFG), replaces the vMF distribution by a mixture of von Mises-Fisher distributions to capture multiple important areas of directions that are far from each other. The second variant, named power spherical sliced fused Gromov Wasserstein (PSSFG), replaces the vMF distribution by a power spherical distribution to improve the sampling time of the vMF distribution in high dimension settings. We then apply the new discrepancies to the RAE framework to achieve its new variants. Finally, we conduct extensive experiments to show that the new autoencoders have favorable performance in learning latent manifold structure, image generation, and reconstruction.</p>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#0048BA"><a href="https://neurips.cc/" target="_blank" rel="noopener noreferrer">NeurIPS</a></abbr></div>

        <!-- Entry bib key -->
        <div id="nguyen2021structured" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Structured Dropout Variational Inference for Bayesian Neural Networks</div>
          <!-- Author -->
          <div class="author">
          

          Son Nguyen, Duong Nguyen, Khai Nguyen, and
            <span class="more-authors" title="click to view 3 more authors" onclick="
                  var element = $(this);
                  element.attr('title', '');
                  var more_authors_text = element.text() == '3 more authors' ? 'Khoat Than, Hung Bui, Nhat Ho' : '3 more authors';
                  var cursorPosition = 0;
                  var textAdder = setInterval(function(){
                    element.text(more_authors_text.substring(0, cursorPosition + 1));
                    if (++cursorPosition == more_authors_text.length){
                      clearInterval(textAdder);
                    }
                }, '10');
                ">3 more authors</span>
</div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>Advances in Neural Information Processing Systems</em> 28–30 mar 2021
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="https://proceedings.NeurIPS.cc/paper/2021/file/80a160ff31266be2f93012a2a3eca713-Paper.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Approximate inference in Bayesian deep networks exhibits a dilemma of how to yield high fidelity posterior approximations while maintaining computational efficiency and scalability. We tackle this challenge by introducing a novel variational structured approximation inspired by the Bayesian interpretation of Dropout regularization. Concretely, we focus on the inflexibility of the factorized structure in Dropout posterior and then propose an improved method called Variational Structured Dropout (VSD). VSD employs an orthogonal transformation to learn a structured representation on the variational Gaussian noise with plausible complexity, and consequently induces statistical dependencies in the approximate posterior. Theoretically, VSD successfully addresses the pathologies of previous Variational Dropout methods and thus offers a standard Bayesian justification. We further show that VSD induces an adaptive regularization term with several desirable properties which contribute to better generalization. Finally, we conduct extensive experiments on standard benchmarks to demonstrate the effectiveness of VSD over state-of-the-art variational methods on predictive accuracy, uncertainty estimation, and out-of-distribution detection.</p>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">Preprint</abbr></div>

        <!-- Entry bib key -->
        <div id="nguyen2021model" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Model Fusion of Heterogeneous Neural Networks via Cross-Layer Alignment</div>
          <!-- Author -->
          <div class="author">
          

          Dang Nguyen, Khai Nguyen, <a href="http://dinhphung.ml/" target="_blank" rel="noopener noreferrer">Dinh Phung</a>, and
            <span class="more-authors" title="click to view 2 more authors" onclick="
                  var element = $(this);
                  element.attr('title', '');
                  var more_authors_text = element.text() == '2 more authors' ? 'Hung Bui, Nhat Ho' : '2 more authors';
                  var cursorPosition = 0;
                  var textAdder = setInterval(function(){
                    element.text(more_authors_text.substring(0, cursorPosition + 1));
                    if (++cursorPosition == more_authors_text.length){
                      clearInterval(textAdder);
                    }
                }, '10');
                ">2 more authors</span>
</div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>arXiv preprint arXiv:2110.15538</em> 28–30 mar 2021
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="https://arxiv.org/pdf/2110.15538.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Layer-wise model fusion via optimal transport, named OTFusion, applies soft neuron association for unifying different pre-trained networks to save computational resources. While enjoying its success, OTFusion requires the input networks to have the same number of layers. To address this issue, we propose a novel model fusion framework, named CLAFusion, to fuse neural networks with a different number of layers, which we refer to as heterogeneous neural networks, via cross-layer alignment. The cross-layer alignment problem, which is an unbalanced assignment problem, can be solved efficiently using dynamic programming. Based on the cross-layer alignment, our framework balances the number of layers of neural networks before applying layer-wise model fusion. Our experiments indicate that CLAFusion, with an extra finetuning process, improves the accuracy of residual networks on CIFAR10 and CIFAR100 datasets. Furthermore, we explore its practical usage for model compression and knowledge distillation when applying to the teacher-student setting.</p>
          </div>
        </div>
      </div>
</li>
</ol>


</div>

          </article>

        </div>

    </div>

    <!-- Footer -->    
    <footer class="fixed-bottom">
      <div class="container mt-0">
        © Copyright 2023 Tuan  Le Duc Anh. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="noopener noreferrer">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" target="_blank" rel="noopener noreferrer">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="noopener noreferrer">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="noopener noreferrer">Unsplash</a>.

      </div>
    </footer>

    <!-- JavaScripts -->
    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    <!-- Masonry & imagesLoaded -->
  <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script defer src="/assets/js/masonry.js" type="text/javascript"></script>
    
  <!-- Medium Zoom JS -->
  <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script>
  <script defer src="/assets/js/zoom.js"></script><!-- Load Common JS -->
  <script defer src="/assets/js/common.js"></script>

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    
  </body>
</html>
